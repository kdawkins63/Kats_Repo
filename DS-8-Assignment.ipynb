Classification with Python
Estimated time needed: 25 minutes

Objectives
After completing this lab you will be able to:

Confidently create classification models

In this notebook we try to practice all the classification algorithms that we learned in this course.

We load a dataset using Pandas library, apply the following algorithms, and find the best one for this specific dataset by accuracy evaluation methods.

Let's first load required libraries:

import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline

/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
About dataset
This dataset is about the performance of basketball teams. The cbb.csv data set includes performance data about five seasons of 354 basketball teams. It includes the following fields:

Field	Description
TEAM	The Division I college basketball school
CONF	The Athletic Conference in which the school participates in (A10 = Atlantic 10, ACC = Atlantic Coast Conference, AE = America East, Amer = American, ASun = ASUN, B10 = Big Ten, B12 = Big 12, BE = Big East, BSky = Big Sky, BSth = Big South, BW = Big West, CAA = Colonial Athletic Association, CUSA = Conference USA, Horz = Horizon League, Ivy = Ivy League, MAAC = Metro Atlantic Athletic Conference, MAC = Mid-American Conference, MEAC = Mid-Eastern Athletic Conference, MVC = Missouri Valley Conference, MWC = Mountain West, NEC = Northeast Conference, OVC = Ohio Valley Conference, P12 = Pac-12, Pat = Patriot League, SB = Sun Belt, SC = Southern Conference, SEC = South Eastern Conference, Slnd = Southland Conference, Sum = Summit League, SWAC = Southwestern Athletic Conference, WAC = Western Athletic Conference, WCC = West Coast Conference)
G	Number of games played
W	Number of games won
ADJOE	Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions) a team would have against the average Division I defense)
ADJDE	Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions) a team would have against the average Division I offense)
BARTHAG	Power Rating (Chance of beating an average Division I team)
EFG_O	Effective Field Goal Percentage Shot
EFG_D	Effective Field Goal Percentage Allowed
TOR	Turnover Percentage Allowed (Turnover Rate)
TORD	Turnover Percentage Committed (Steal Rate)
ORB	Offensive Rebound Percentage
DRB	Defensive Rebound Percentage
FTR	Free Throw Rate (How often the given team shoots Free Throws)
FTRD	Free Throw Rate Allowed
2P_O	Two-Point Shooting Percentage
2P_D	Two-Point Shooting Percentage Allowed
3P_O	Three-Point Shooting Percentage
3P_D	Three-Point Shooting Percentage Allowed
ADJ_T	Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team that wants to play at an average Division I tempo)
WAB	Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)
POSTSEASON	Round where the given team was eliminated or where their season ended (R68 = First Four, R64 = Round of 64, R32 = Round of 32, S16 = Sweet Sixteen, E8 = Elite Eight, F4 = Final Four, 2ND = Runner-up, Champion = Winner of the NCAA March Madness Tournament for that given year)
SEED	Seed in the NCAA March Madness Tournament
YEAR	Season
Load Data From CSV File
Let's load the dataset [NB Need to provide link to csv file]

df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%206/cbb.csv')
df.head()
TEAM	CONF	G	W	ADJOE	ADJDE	BARTHAG	EFG_O	EFG_D	TOR	...	FTRD	2P_O	2P_D	3P_O	3P_D	ADJ_T	WAB	POSTSEASON	SEED	YEAR
0	North Carolina	ACC	40	33	123.3	94.9	0.9531	52.6	48.1	15.4	...	30.4	53.9	44.6	32.7	36.2	71.7	8.6	2ND	1.0	2016
1	Villanova	BE	40	35	123.1	90.9	0.9703	56.1	46.7	16.3	...	30.0	57.4	44.1	36.2	33.9	66.7	8.9	Champions	2.0	2016
2	Notre Dame	ACC	36	24	118.3	103.3	0.8269	54.0	49.5	15.3	...	26.0	52.9	46.5	37.4	36.9	65.5	2.3	E8	6.0	2016
3	Virginia	ACC	37	29	119.9	91.0	0.9600	54.8	48.4	15.1	...	33.4	52.6	46.3	40.3	34.7	61.9	8.6	E8	1.0	2016
4	Kansas	B12	37	32	120.9	90.4	0.9662	55.7	45.1	17.8	...	37.3	52.7	43.4	41.3	32.5	70.1	11.6	E8	1.0	2016
5 rows × 24 columns

df.shape
(1406, 24)
Add Column
Next we'll add a column that will contain "true" if the wins above bubble are over 7 and "false" if not. We'll call this column Win Index or "windex" for short.

df['windex'] = np.where(df.WAB > 7, 'True', 'False')

Next we'll filter the data set to the teams that made the Sweet Sixteen, the Elite Eight, and the Final Four in the post season. We'll also create a new dataframe that will hold the values with the new column.

df1 = df.loc[df['POSTSEASON'].str.contains('F4|S16|E8', na=False)]
df1.head()
TEAM	CONF	G	W	ADJOE	ADJDE	BARTHAG	EFG_O	EFG_D	TOR	...	2P_O	2P_D	3P_O	3P_D	ADJ_T	WAB	POSTSEASON	SEED	YEAR	windex
2	Notre Dame	ACC	36	24	118.3	103.3	0.8269	54.0	49.5	15.3	...	52.9	46.5	37.4	36.9	65.5	2.3	E8	6.0	2016	False
3	Virginia	ACC	37	29	119.9	91.0	0.9600	54.8	48.4	15.1	...	52.6	46.3	40.3	34.7	61.9	8.6	E8	1.0	2016	True
4	Kansas	B12	37	32	120.9	90.4	0.9662	55.7	45.1	17.8	...	52.7	43.4	41.3	32.5	70.1	11.6	E8	1.0	2016	True
5	Oregon	P12	37	30	118.4	96.2	0.9163	52.3	48.9	16.1	...	52.6	46.1	34.4	36.2	69.0	6.7	E8	1.0	2016	False
6	Syracuse	ACC	37	23	111.9	93.6	0.8857	50.0	47.3	18.1	...	47.2	48.1	36.0	30.7	65.5	-0.3	F4	10.0	2016	False
5 rows × 25 columns

df1['POSTSEASON'].value_counts()
S16    32
E8     16
F4      8
Name: POSTSEASON, dtype: int64
32 teams made it into the Sweet Sixteen, 16 into the Elite Eight, and 8 made it into the Final Four over 5 seasons.

Lets plot some columns to underestand the data better:

# notice: installing seaborn might takes a few minutes
!conda install -c anaconda seaborn -y
Collecting package metadata (current_repodata.json): done
Solving environment: done

# All requested packages already installed.

import seaborn as sns

bins = np.linspace(df1.BARTHAG.min(), df1.BARTHAG.max(), 10)
g = sns.FacetGrid(df1, col="windex", hue="POSTSEASON", palette="Set1", col_wrap=6)
g.map(plt.hist, 'BARTHAG', bins=bins, ec="k")

g.axes[-1].legend()
plt.show()

bins = np.linspace(df1.ADJOE.min(), df1.ADJOE.max(), 10)
g = sns.FacetGrid(df1, col="windex", hue="POSTSEASON", palette="Set1", col_wrap=2)
g.map(plt.hist, 'ADJOE', bins=bins, ec="k")

g.axes[-1].legend()
plt.show()

Pre-processing: Feature selection/extraction
Lets look at how Adjusted Defense Efficiency plots
bins = np.linspace(df1.ADJDE.min(), df1.ADJDE.max(), 10)
g = sns.FacetGrid(df1, col="windex", hue="POSTSEASON", palette="Set1", col_wrap=2)
g.map(plt.hist, 'ADJDE', bins=bins, ec="k")
g.axes[-1].legend()
plt.show()
​
We see that this data point doesn't impact the ability of a team to get into the Final Four.

Convert Categorical features to numerical values
Lets look at the postseason:

df1.groupby(['windex'])['POSTSEASON'].value_counts(normalize=True)
windex  POSTSEASON
False   S16           0.605263
        E8            0.263158
        F4            0.131579
True    S16           0.500000
        E8            0.333333
        F4            0.166667
Name: POSTSEASON, dtype: float64
13% of teams with 6 or less wins above bubble make it into the final four while 17% of teams with 7 or more do.

Lets convert wins above bubble (winindex) under 7 to 0 and over 7 to 1:

df1['windex'].replace(to_replace=['False','True'], value=[0,1],inplace=True)
df1.head()


	TEAM	CONF	G	W	ADJOE	ADJDE	BARTHAG	EFG_O	EFG_D	TOR	...	2P_O	2P_D	3P_O	3P_D	ADJ_T	WAB	POSTSEASON	SEED	YEAR	windex
2	Notre Dame	ACC	36	24	118.3	103.3	0.8269	54.0	49.5	15.3	...	52.9	46.5	37.4	36.9	65.5	2.3	E8	6.0	2016	0
3	Virginia	ACC	37	29	119.9	91.0	0.9600	54.8	48.4	15.1	...	52.6	46.3	40.3	34.7	61.9	8.6	E8	1.0	2016	1
4	Kansas	B12	37	32	120.9	90.4	0.9662	55.7	45.1	17.8	...	52.7	43.4	41.3	32.5	70.1	11.6	E8	1.0	2016	1
5	Oregon	P12	37	30	118.4	96.2	0.9163	52.3	48.9	16.1	...	52.6	46.1	34.4	36.2	69.0	6.7	E8	1.0	2016	0
6	Syracuse	ACC	37	23	111.9	93.6	0.8857	50.0	47.3	18.1	...	47.2	48.1	36.0	30.7	65.5	-0.3	F4	10.0	2016	0
5 rows × 25 columns

Feature selection
Let's define feature sets, X:

X = df1[['G', 'W', 'ADJOE', 'ADJDE', 'BARTHAG', 'EFG_O', 'EFG_D',
       'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P_O', '2P_D', '3P_O',
       '3P_D', 'ADJ_T', 'WAB', 'SEED', 'windex']]
X[0:5]
G	W	ADJOE	ADJDE	BARTHAG	EFG_O	EFG_D	TOR	TORD	ORB	...	FTR	FTRD	2P_O	2P_D	3P_O	3P_D	ADJ_T	WAB	SEED	windex
2	36	24	118.3	103.3	0.8269	54.0	49.5	15.3	14.8	32.7	...	32.9	26.0	52.9	46.5	37.4	36.9	65.5	2.3	6.0	0
3	37	29	119.9	91.0	0.9600	54.8	48.4	15.1	18.8	29.9	...	32.1	33.4	52.6	46.3	40.3	34.7	61.9	8.6	1.0	1
4	37	32	120.9	90.4	0.9662	55.7	45.1	17.8	18.5	32.2	...	38.6	37.3	52.7	43.4	41.3	32.5	70.1	11.6	1.0	1
5	37	30	118.4	96.2	0.9163	52.3	48.9	16.1	20.2	34.1	...	40.3	32.0	52.6	46.1	34.4	36.2	69.0	6.7	1.0	0
6	37	23	111.9	93.6	0.8857	50.0	47.3	18.1	20.4	33.5	...	35.4	28.0	47.2	48.1	36.0	30.7	65.5	-0.3	10.0	0
5 rows × 21 columns

What are our lables? Round where the given team was eliminated or where their season ended (R68 = First Four, R64 = Round of 64, R32 = Round of 32, S16 = Sweet Sixteen, E8
= Elite Eight, F4 = Final Four, 2ND = Runner-up, Champion = Winner of the NCAA March Madness Tournament for that given year)|

y = df1['POSTSEASON'].values
y[0:5]
array(['E8', 'E8', 'E8', 'E8', 'F4'], dtype=object)
Normalize Data
Data Standardization gives data zero mean and unit variance (technically should be done after train test split )

X= preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]
/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.
  return self.partial_fit(X, y)
array([[-0.43331874, -1.26140173,  0.28034482,  2.74329908, -2.45717765,
         0.10027963,  0.94171924, -1.16188145, -1.71391372,  0.12750511,
         1.33368704, -0.4942211 , -0.87998988,  0.02784185,  0.00307239,
         0.22576157,  1.59744386, -1.12106011, -1.0448016 ,  0.49716104,
        -0.6882472 ],
       [ 0.40343468,  0.35874728,  0.64758014, -0.90102957,  1.127076  ,
         0.39390887,  0.38123706, -1.29466791, -0.03522254, -0.62979797,
        -1.31585883, -0.68542235,  0.55458056, -0.07167795, -0.0829545 ,
         1.32677295,  0.65081046, -2.369021  ,  0.98050611, -1.14054592,
         1.45296631],
       [ 0.40343468,  1.33083669,  0.87710222, -1.0788017 ,  1.29403598,
         0.72424177, -1.30020946,  0.49794919, -0.16112438, -0.00772758,
        -0.27908001,  0.86808783,  1.31063795, -0.03850468, -1.33034432,
         1.70643205, -0.29582294,  0.47355659,  1.94493836, -1.14054592,
         1.45296631],
       [ 0.40343468,  0.68277708,  0.30329703,  0.63966222, -0.04972253,
        -0.52368251,  0.63600169, -0.63073565,  0.55231938,  0.50615665,
         0.71929959,  1.2743905 ,  0.28317534, -0.07167795, -0.16898138,
        -0.91321572,  1.29624232,  0.0922352 ,  0.36969903, -1.14054592,
        -0.6882472 ],
       [ 0.40343468, -1.58543153, -1.18859646, -0.13068368, -0.87375079,
        -1.36786658, -0.17924511,  0.69712887,  0.63625394,  0.34387742,
         2.56246194,  0.10328282, -0.49226814, -1.8630343 ,  0.69128747,
        -0.30576117, -1.07034117, -1.12106011, -1.88064288,  1.80732661,
        -0.6882472 ]])

Training and Validation
Split the data into Training and Validation data.

# We split the X into train and test to find the best k
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Validation set:', X_val.shape,  y_val.shape)
Train set: (44, 21) (44,)
Validation set: (12, 21) (12,)
Classification
Now, it is your turn, use the training set to build an accurate model. Then use the validation set to report the accuracy of the model You should use the following algorithm:

K Nearest Neighbor(KNN)
Decision Tree
Support Vector Machine
Logistic Regression

K Nearest Neighbor(KNN)
Question 1 Build a KNN model using a value of k equals five, find the accuracy on the validation data (X_val and y_val)

You can use  accuracy_score

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

k=5

# Train Model and Predict
neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)
y_hat = neigh.predict(X_val)

Val_accuracy = accuracy_score(y_val, y_hat)
Val_accuracy

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

k=5

# Train Model and Predict
neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)
y_hat = neigh.predict(X_val)

Val_accuracy = accuracy_score(y_val, y_hat)
Val_accuracy

0.6666666666666666

yhat = neigh.predict(X_val)
yhat[0:5]

array(['S16', 'S16', 'S16', 'E8', 'E8'], dtype=object)

print("Train set Accuracy: ", accuracy_score(y_train, neigh.predict(X_train)))
print("Validate set Accuracy: ", accuracy_score(y_val, yhat))

Train set Accuracy:  0.6363636363636364
Validate set Accuracy:  0.6666666666666666

Question 2 Determine and print the accuracy for the first 15 values of k on the validation data:

Ks = 15
mean_acc = np.zeros((Ks-1))
std_acc = [Ks-1]
    
for i in range(1,Ks):
    neigh = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)
    y_hat = neigh.predict(X_val)
    mean_acc[i-1] = accuracy_score(y_val, y_hat)
mean_acc

array([0.33333333, 0.33333333, 0.5       , 0.58333333, 0.66666667,
       0.58333333, 0.58333333, 0.66666667, 0.58333333, 0.58333333,
       0.58333333, 0.5       , 0.58333333, 0.58333333])

Decision Tree
The following lines of code fit a DecisionTreeClassifier:

from sklearn.tree import DecisionTreeClassifier
Question 3 Determine the minumum value for the parameter max_depth that improves results

MAX_DEPTH = 11

accuracy = np.zeros((MAX_DEPTH - 1))

for i in range(1, MAX_DEPTH):
    tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = i)
    tree.fit(X_train,y_train)
    predtree = tree.predict(X_val)

accuracy[i - 1] = accuracy_score(y_val, predtree)
accuracy

array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.41666667])
Support Vector Machine
Question 4 Train the support vector machine model and determine the accuracy on the validation data for each kernel. 
Find the kernel (linear, poly, rbf, sigmoid) that provides the best score on the validation data and train a SVM using it.

from sklearn import svm
from sklearn.metrics import f1_score
from sklearn.metrics import jaccard_similarity_score

import scipy

kernel_list = ['linear','poly','rbf','sigmoid']
Jaccard_out_list = []
F1_out_list = []

for model in kernel_list:
    clf = svm.SVC(kernel=model)
    clf.fit(X_train, y_train)
    y_hat = clf.predict(X_val)
    f1 = f1_score(y_val,y_hat, average='weighted')
    F1_out_list.append(f1)
    j1 = jaccard_similarity_score(y_val,y_hat)
    Jaccard_out_list.append(j1)

print(kernel_list,'\\n', Jaccard_out_list,'\\n', F1_out_list)
print('Poly provides the best score on the validation data with a Jaccard Score of ' , Jaccard_out_list[1] , ' and an F1 Score of ' , F1_out_list[1], '.')

['linear', 'poly', 'rbf', 'sigmoid'] \n [0.25, 0.6666666666666666, 0.5833333333333334, 0.5] \n [0.24216524216524218, 0.5333333333333333, 0.49122807017543857, 0.44444444444444436]
Poly provides the best score on the validation data with a Jaccard Score of  0.6666666666666666  and an F1 Score of  0.5333333333333333 .

Logistic Regression
Question 5 Train a logistic regression model and determine the accuracy of the validation data (set C=0.01)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
y_hat = LR.predict(X_val)
y_hat_prob = LR.predict_proba(X_val)
print('Jaccard Score: ', jaccard_similarity_score(y_val, y_hat), '\\n Accuracy Score: ', accuracy_score(y_val, y_hat), '\\n Alt Accuracy Score: ', LR.score(X_val,y_val), '\\n Log Loss Score: ', log_loss(y_val, y_hat_prob))

Jaccard Score:  0.5833333333333334 \n Accuracy Score:  0.5833333333333334 \n Alt Accuracy Score:  0.5833333333333334 \n Log Loss Score:  1.095461062326229

Model Evaluation using Test set
#from sklearn.metrics import f1_score
# for f1_score please set the average parameter to 'micro'
#from sklearn.metrics import log_loss
​
from sklearn.metrics import f1_score
# for f1_score please set the average parameter to 'micro'\n
from sklearn.metrics import log_loss
def jaccard_index(predictions, true):
    if (len(predictions) == len(true)):
        intersect = 0;
        for x,y in zip(predictions, true):
            if (x == y):
                intersect += 1
        return intersect / (len(predictions) + len(true) - intersect)
    else:
        return -1

Question 5 Calculate the F1 score and Jaccard score for each model from above. Use the Hyperparameter that performed best on the validation data. For f1_score please set the average parameter to 'micro'.

Load Test set for evaluation
test_df = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0120ENv3/Dataset/ML0101EN_EDX_skill_up/basketball_train.csv',error_bad_lines=False)
test_df.head()

TEAM	CONF	G	W	ADJOE	ADJDE	BARTHAG	EFG_O	EFG_D	TOR	...	FTRD	2P_O	2P_D	3P_O	3P_D	ADJ_T	WAB	POSTSEASON	SEED	YEAR
0	North Carolina	ACC	40	33	123.3	94.9	0.9531	52.6	48.1	15.4	...	30.4	53.9	44.6	32.7	36.2	71.7	8.6	2ND	1.0	2016
1	Villanova	BE	40	35	123.1	90.9	0.9703	56.1	46.7	16.3	...	30.0	57.4	44.1	36.2	33.9	66.7	8.9	Champions	2.0	2016
2	Notre Dame	ACC	36	24	118.3	103.3	0.8269	54.0	49.5	15.3	...	26.0	52.9	46.5	37.4	36.9	65.5	2.3	E8	6.0	2016
3	Virginia	ACC	37	29	119.9	91.0	0.9600	54.8	48.4	15.1	...	33.4	52.6	46.3	40.3	34.7	61.9	8.6	E8	1.0	2016
4	Kansas	B12	37	32	120.9	90.4	0.9662	55.7	45.1	17.8	...	37.3	52.7	43.4	41.3	32.5	70.1	11.6	E8	1.0	2016
5 rows × 24 columns

test_df['windex'] = np.where(test_df.WAB > 7, 'True', 'False')
test_df1 = test_df[test_df['POSTSEASON'].str.contains('F4|S16|E8', na=False)]
test_Feature = test_df1[['G', 'W', 'ADJOE', 'ADJDE', 'BARTHAG', 'EFG_O', 'EFG_D',
       'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P_O', '2P_D', '3P_O',
       '3P_D', 'ADJ_T', 'WAB', 'SEED', 'windex']]
test_Feature['windex'].replace(to_replace=['False','True'], value=[0,1],inplace=True)
test_X=test_Feature
test_X= preprocessing.StandardScaler().fit(test_X).transform(test_X)
test_X[0:5]

array([[-4.08074446e-01, -1.10135297e+00,  3.37365934e-01,
         2.66479976e+00, -2.46831661e+00,  2.13703245e-01,
         9.44090550e-01, -1.19216365e+00, -1.64348924e+00,
         1.45405982e-02,  1.29523097e+00, -6.23533182e-01,
        -9.31788560e-01,  1.42784371e-01,  1.68876201e-01,
         2.84500844e-01,  1.62625961e+00, -8.36649260e-01,
        -9.98500539e-01,  4.84319174e-01, -6.77003200e-01],
       [ 3.63958290e-01,  3.26326807e-01,  7.03145068e-01,
        -7.13778644e-01,  1.07370841e+00,  4.82633172e-01,
         4.77498943e-01, -1.32975879e+00, -6.86193316e-02,
        -7.35448152e-01, -1.35447914e+00, -8.06829025e-01,
         3.41737757e-01,  4.96641291e-02,  9.40576311e-02,
         1.37214061e+00,  6.93854620e-01, -2.00860931e+00,
         9.80549967e-01, -1.19401460e+00,  1.47709789e+00],
       [ 3.63958290e-01,  1.18293467e+00,  9.31757027e-01,
        -8.78587347e-01,  1.23870131e+00,  7.85179340e-01,
        -9.22275877e-01,  5.27775662e-01, -1.86734575e-01,
        -1.19385964e-01, -3.17636057e-01,  6.82449703e-01,
         1.01292055e+00,  8.07042098e-02, -9.90811637e-01,
         1.74718880e+00, -2.38550367e-01,  6.60855252e-01,
         1.92295497e+00, -1.19401460e+00,  1.47709789e+00],
       [ 3.63958290e-01,  6.11862762e-01,  3.60227129e-01,
         7.14563447e-01, -8.92254236e-02, -3.57772849e-01,
         6.89586037e-01, -6.41783067e-01,  4.82585136e-01,
         3.89534973e-01,  6.80805434e-01,  1.07195337e+00,
         1.00800346e-01,  4.96641291e-02,  1.92390609e-02,
        -8.40643737e-01,  1.32958529e+00,  3.02756347e-01,
         3.83693465e-01, -1.19401460e+00, -6.77003200e-01],
       [ 3.63958290e-01, -1.38688893e+00, -1.12575060e+00,
         3.92401673e-04, -9.03545224e-01, -1.13094639e+00,
         1.09073363e-02,  7.34168378e-01,  5.61328631e-01,
         2.28823098e-01,  2.52408203e+00, -5.07336709e-02,
        -5.87592258e-01, -1.62650023e+00,  7.67424763e-01,
        -2.40566627e-01, -1.00142717e+00, -8.36649260e-01,
        -1.81525154e+00,  1.82698619e+00, -6.77003200e-01]])

test_y = test_df1['POSTSEASON'].values
test_y[0:5]

array(['E8', 'E8', 'E8', 'E8', 'F4'], dtype=object)
KNN

test_y_KNN = neigh.predict(test_X)

print("F1 score of KNN is = ", f1_score(test_y, test_y_KNN, average = "micro"))
print("Jaccard Score of KNN is = ", jaccard_similarity_score(test_y, test_y_KNN))

F1 score of KNN is =  0.5714285714285714
Jaccard Score of KNN is =  0.5714285714285714

Decision Tree

test_y_DT = tree.predict(test_X)

print("F1 score of Decision Tree is=",f1_score(test_y, test_y_DT, average="micro"))
print("Jaccard Score of Decision Tree is=",jaccard_similarity_score(test_y, test_y_DT))

F1 score of Decision Tree is= 0.7714285714285715
Jaccard Score of Decision Tree is= 0.7714285714285715
SVM

clf = svm.SVC(kernel='poly')
clf.fit(X_train, y_train)
test_y_SVM = clf.predict(test_X)
print("F1 score of SVM is=",f1_score(test_y, test_y_SVM, average="micro"))
print("Jaccard Score of SVM is=",jaccard_similarity_score(test_y, test_y_SVM))
F1 score of SVM is= 0.6857142857142857
Jaccard Score of SVM is= 0.6857142857142857

Logistic Regression

test_y_LR = tree.predict(test_X)
test_y_LL = LR.predict_proba(test_X)
print("F1 score of Decision Tree is=",f1_score(test_y, test_y_LR, average="micro"))
print("Jaccard Score of Decision Tree is=",jaccard_similarity_score(test_y, test_y_LR))
print("Log Loss is=",log_loss(test_y, test_y_LL))
F1 score of Decision Tree is= 0.7714285714285715
Jaccard Score of Decision Tree is= 0.7714285714285715
Log Loss is= 1.0371869905927804

Report
You should be able to report the accuracy of the built model using different evaluation metrics:

Algorithm	Accuracy	Jaccard	F1-score	LogLoss
KNN	?	?	?	NA
Decision Tree	?	?	?	NA
SVM	?	?	?	NA
LogisticRegression	?	?	?	?
Something to keep in mind when creating models to predict the results of basketball tournaments or sports in general is that is quite hard due to so many factors influencing the game. Even in sports betting an accuracy of 55% and over is considered good as it indicates profits.

